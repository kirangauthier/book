{
  "hash": "830266d9809fc00bf968e066182e7635",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Matching\"\nshare:\n  permalink: \"https://book.martinez.fyi/matching.html\"\n  description: \"Business Data Science: What Does it Mean to Be Data-Driven?\"\n  linkedin: true\n  email: true\n  mastodon: true\nauthor:\n  - name: ??\n  - name: Ignacio Martinez  \n---\n\n\nIn the realm of causal inference, matching stands out as a powerful and popular\nstatistical technique. Its primary goal? To construct a valid comparison group\nby pairing treated units with untreated units that are as similar as possible\nbased on observable characteristics. This chapter will dive deep into the world\nof matching, exploring its mechanics, applications, and limitations.\n\n## The Bootcamp Conundrum\n\nImagine a tech company, eager to propel its engineers forward, rolls out a shiny\nnew AI bootcamp. Yet, due to scheduling quirks, the bootcamp ends up heavily\nskewed towards senior engineers – those with five or more years under their\nbelts. This poses a classic causal inference challenge.\n\nIn the potential outcomes framework, we envision each engineer with two possible\ncareer paths: one if they attend the bootcamp ($Y_1$), another if they don't\n($Y_0$). The rub, of course, is that we only witness one reality per engineer.\n\nThe non-random enrollment in our bootcamp muddies the waters. Simply comparing\nbootcamp graduates to non-participants would be like judging a footrace where\none runner had a head start. The bootcamp group, on average, boasts more\nexperience – a factor we know can independently turbocharge careers.\n\n## Matching to the Rescue\n\nTo level the playing field, we construct a matched control group. For each\nbootcamp attendee, we seek out a non-attendee with a similar experience level.\nBy comparing outcomes within these matched pairs, we can tease out the\nbootcamp's true impact, disentangling it from the effects of experience.\n\nYet, the plot thickens. What if bootcamp participation wasn't solely about\nexperience? In a global company, time zones could play a role. Attending a\nbootcamp during US business hours is far more convenient for an engineer in New\nYork than one in Tokyo. Here, time zone becomes a confounder, potentially\ninfluencing both bootcamp attendance and career trajectory.\n\nOne might try to match on both experience and location, but this quickly becomes\nunwieldy as more factors enter the picture. The elegant solution is to estimate\na propensity score – the probability of each engineer attending the bootcamp\nbased on their various characteristics. By matching on this propensity score, we\ncreate comparable groups, even when those groups differ on a multitude of\nindividual attributes.\n\n## The Mechanics of Matching\n\nMatching typically involves four key steps:\n\n1.  Choose a distance measure to quantify the similarity between units.\n2.  Match treated units to untreated units based on this distance measure.\n3.  Assess the quality of the matches and iterate if necessary.\n4.  Estimate treatment effects using the matched sample.\n\nLet's explore two common distance measures in detail: Mahalanobis distance and\npropensity scores.\n\n\n### Mahalanobis Distance: Accounting for Covariate Relationships\n\nMahalanobis distance is a multivariate measure of the distance between a point\nand the center of a distribution. It's particularly useful in matching because\nit accounts for the correlations between variables.\n\nKey features of Mahalanobis distance include:\n\n  - Scale-invariance: It's unaffected by the scale of measurement.\n  - Covariance consideration: It accounts for relationships between variables.\n  - Euclidean equivalence: For uncorrelated variables with unit variance, it\n    reduces to Euclidean distance.\n\nMathematically, the Mahalanobis distance between two points $x$ and $y$ in\np-dimensional space is:\n\n$$D_M(x,y) = \\sqrt{(x-y)^T S^{-1} (x-y)}$$\nWhere $S$ is the covariance matrix of\nthe variables.\n\n### Propensity Scores: Collapsing Dimensions\n\nThe propensity score represents the probability of receiving treatment given\nobserved covariates, often estimated using logistic regression. Key features of\npropensity scores include:\n\n  - Dimension reduction: They collapse multiple covariates into a single score.\n  - Balance assessment: They make it easier to check balance on a single\n    dimension.\n  - Interpretability: They represent the probability of treatment.\n\nThe propensity score is given by: $$ e(X) = P(T=1|X)$$\nWhere $T$ is the treatment indicator and $X$ is the vector of covariates.\n\n### Key Differences Between Mahalanobis Distance and Propensity Score\n\n| Feature                 | Mahalanobis Distance                 | Propensity Score                                    |\n| ----------------------- | ------------------------------------ | --------------------------------------------------- |\n| Dimensionality          | Operates in original covariate space | Reduces matching to a single dimension              |\n| Interpretation          | Measures multivariate similarity     | Represents probability of treatment                 |\n| Covariate relationships | Explicitly accounts for covariance   | Implicitly captures relationships through the model |\n| Model specification     | Doesn't require a model              | Can be sensitive to estimation method               |\n| Categorical variables   | Can struggle with them               | Naturally incorporates them                         |\n| Curse of dimensionality | Can suffer in high dimensions        | Handles higher dimensions more easily               |\n\n### When to Use Each\n\n  - **Mahalanobis distance:** Ideal when you have few continuous covariates,\n    relationships between covariates are important, and you want to avoid\n    specifying a treatment model.\n  - **Propensity scores:** Better suited when you have many covariates\n    (including categorical ones), the treatment mechanism is of interest, and\n    you want to easily assess balance and overlap.\n\n### Matching Algorithms: Putting Theory into Practice\n\nOnce we've chosen a distance measure, we need an algorithm to perform the actual\nmatching. Three common approaches are:\n\n  - Nearest neighbor matching: Matches each treated unit to the closest\n    untreated unit.\n  - Optimal matching: Minimizes the total distance across all matched pairs.\n  - Full matching: Creates matched sets, each containing at least one treated\n    and one untreated unit.\n\n## The Limits of Matching: Avoiding Matching Charles to Ozzy\n\nAs with any causal inference method, matching is not a magic bullet. It works\nbest when you have the right data to model treatment assignment. Essentially,\nafter matching, whether someone is in the treatment group should be effectively\nrandom.\n\nFor example, in our bootcamp scenario, imagine that participation is largely\nexplained by an engineer's \"grit\" – a trait we cannot directly observe or match\non. If career trajectory is also a function of grit, we might mistakenly\nconclude that the bootcamp has a larger impact than it truly does. Conversely,\nif procrastinators are more likely to participate, we might wrongly infer that\nthe bootcamp hurts career success.\n\nA memorable way to understand this limitation is through the \"Ozzy Osbourne\nConundrum.\" Consider these two individuals:\n\n\n+--------------------------------------------------+--------------------------------------------+\n|                    Charles                       |                    Ozzy                    |\n+:================================================:+:==========================================:+\n|                                                  |                                            |\n| ![](img/charles.webp){width=400px, height=500px} |![](img/ozzy.png){width=400px, height=500px}|\n|                                                  |                                            |\n|                     Male                         |                   Male                     |\n|                                                  |                                            |\n|                 Born in 1948                     |               Born in 1948                 |\n|                                                  |                                            |\n|               Raised in the UK                   |             Raised in the UK               |\n|                                                  |                                            |\n|              Lives in a castle                   |             Lives in a castle              |\n|                                                  |                                            |\n|              Wealthy & famous                    |             Wealthy & famous               |\n+--------------------------------------------------+--------------------------------------------+\n: Matching Charles to Ozzy {#tbl-ozzy_and_charles}\n\nOzzy and Charles share many observable characteristics: they're both males, born\nin 1948, raised in the UK, live in castles, and are wealthy and famous. However,\nOzzy would clearly not be a good match for Charles in most studies. This example\nillustrates how matching on observables can sometimes be misleading.\n\nThe key takeaway? Matching is a powerful tool, but it relies on the assumption\nthat after matching, the remaining differences between groups are essentially\nrandom. If this assumption doesn't hold, our conclusions may be misleading.\n\n## The Propensity Score Paradox: A Critique by King and Nielsen\n\nIn their influential paper, @King_Nielsen_2019 present a compelling critique\nof propensity score matching (PSM). Their findings challenge conventional wisdom\nand offer important insights for practitioners of matching methods.\n\n### The PSM Paradox\n\nAt the heart of King and Nielsen's argument is what they term the \"PSM paradox.\"\nThey demonstrate that under certain conditions, PSM can actually increase\nimbalance, model dependence, and bias. This occurs because PSM approximates a\ncompletely randomized experiment, rather than a more efficient fully blocked\nrandomized experiment.\n\nKey findings include:\n\n1.  Increased Imbalance: As PSM prunes observations to improve balance, it can\n    paradoxically increase imbalance on the original covariates after a certain\n    point.\n2.  Model Dependence: PSM can lead to greater model dependence, meaning that\n    different model specifications can yield substantially different causal\n    estimates.\n3.  Bias: The combination of increased imbalance and model dependence can result\n    in biased causal estimates.\n\n### The Mechanics Behind the Paradox\n\nKing and Nielsen explain that PSM's shortcomings stem from its attempt to\napproximate complete randomization. In contrast, other matching methods aim to\napproximate full blocking, which is generally more efficient and precise.\n\n1.  Information Loss: PSM collapses multi-dimensional covariate information into\n    a single dimension (the propensity score), potentially discarding valuable\n    information.\n2.  Random Pruning: Once PSM achieves its goal of approximate randomization,\n    further pruning of observations becomes essentially random with respect to\n    the original covariates. This random pruning can increase imbalance.\n3.  Dimensionality: The problems with PSM become more pronounced as the number\n    of covariates increases.\n\n### Empirical Evidence\n\nThe authors provide evidence from both simulations and real-world datasets to\nsupport their claims. They show that as PSM prunes more observations, other\nmatching methods (like Mahalanobis distance matching) continue to improve\nbalance, while PSM begins to worsen it.\n\n### Recommendations\n\nBased on their findings, King and Nielsen offer several recommendations:\n\n1.  Avoid PSM for Matching: They suggest using other matching methods that\n    better approximate full blocking, such as Mahalanobis distance matching or\n    coarsened exact matching.\n2.  Use PSM Carefully: If using PSM, researchers should be aware of its\n    limitations and stop pruning before the paradox kicks in.\n3.  Balance Checking: Regardless of the matching method used, researchers should\n    always check covariate balance before and after matching.\n4.  Consider Alternative Uses: While discouraging PSM for matching, the authors\n    note that propensity scores can be useful in other contexts, such as\n    weighting or subclassification.\n\n### Implications for Practice\n\nThis critique has significant implications for how we approach matching in\ncausal inference:\n\n1.  Method Selection: When choosing a matching method, consider how well it\n    approximates full blocking rather than complete randomization.\n2.  Iterative Process: Matching should be an iterative process, with continuous\n    checks on balance and careful consideration of when to stop pruning\n    observations.\n3.  Multidimensional Balance: Pay attention to balance on the original\n    covariates, not just the propensity score.\n4.  Transparency: Given the potential for increased model dependence, it's\n    crucial to be transparent about the matching process and to consider\n    multiple model specifications.\n\n## Practical Examples with MatchIt\n\nThe R package [{MatchIt}](https://kosukeimai.github.io/MatchIt/) provides a\ncomprehensive set of tools for implementing various matching methods. It was\ndeveloped based on the recommendations of [@ho2007matching] for improving\nparametric models through nonparametric preprocessing. \n\nMatchIt supports a wide range of matching techniques, including:\n\n- Exact matching\n- Nearest neighbor matching\n- Optimal matching\n- Full matching\n- Genetic matching\n- Coarsened exact matching\n\n### Cautionary tale: Unmeasured Confounders. \n\nImagine you're a data scientist at the illustrious TechGiant Inc., a company\nthat recently rolled out an intensive AI bootcamp program for its engineers.\nThis ambitious initiative aims to elevate the workforce's skills and propel\ninnovation to new heights. You've been entrusted with a crucial task: to\nevaluate the program's effectiveness by examining its impact on engineers'\nsalaries.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(MatchIt)\nlibrary(dplyr)\nlibrary(ggplot2)\nset.seed(123)\n\n# Generate synthetic data\nn <- 1000\nexperience <- runif(n, 0, 10)  # Years of experience\nmotivation <- rnorm(n)  # Unobserved motivation level\nbootcamp <- rbinom(n, 1, plogis(-0.3 * experience + 0.5 * motivation))  # Bootcamp participation\nsalary_increase <- 2000 * bootcamp + 1000 * experience - 3000 * motivation + rnorm(n, 0, 5000)\n\n# True average treatment effect is $2000\n\ndata <- data.frame(experience = experience, bootcamp = bootcamp, salary_increase = salary_increase)\n\n# Naive estimate\nnaive_model <- lm(salary_increase ~ bootcamp, data = data)\nnaive_ate <- coef(naive_model)[\"bootcamp\"]\n\n# Matching on experience (ignoring unobserved motivation)\nm.out <- matchit(bootcamp ~ experience, data = data, method = \"nearest\", ratio = 1)\nmatched_data <- match.data(m.out)\n\n# Estimate ATE on matched data\nmatched_model <- lm(salary_increase ~ bootcamp, data = matched_data, weights = weights)\nmatched_ate <- coef(matched_model)[\"bootcamp\"]\n\n# Print results\ncat(\"True ATE: $2000\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nTrue ATE: $2000\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Naive ATE estimate:\", round(naive_ate, 2), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nNaive ATE estimate: -1205.13 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Matched ATE estimate:\", round(matched_ate, 2), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nMatched ATE estimate: 880.24 \n```\n\n\n:::\n\n```{.r .cell-code}\n# Visualize results\nggplot(data, aes(x = experience, y = salary_increase, color = factor(bootcamp))) +\n  geom_point(alpha = 0.5) +\n  geom_smooth(method = \"lm\", se = FALSE) +\n  labs(title = \"AI Bootcamp Effect on Salary Increase\",\n       subtitle = \"True effect is positive, but observed relationship appears negative\",\n       x = \"Years of Experience\",\n       y = \"Salary Increase ($)\",\n       color = \"Bootcamp Participation\") +\n  theme_minimal()\n```\n\n::: {.cell-output-display}\n![](matching_files/figure-html/unmeasure_confounders-1.png){width=672}\n:::\n:::\n\n\nWhat's happening in this scenario? Let's break it down:\n\n\n1.  **The True Impact:** In reality, the bootcamp program is a success. It\n    genuinely enhances skills and, consequently, leads to higher salary\n    increases.\n2.  **Experience and Participation:** Less experienced engineers are more likely\n    to enroll in the bootcamp, perhaps viewing it as a way to bridge the gap\n    with their seasoned colleagues.\n3.  **Motivation as a Hidden Factor:** These same less experienced engineers,\n    driven to prove their worth, also tend to be highly motivated in their\n    day-to-day work.\n4.  **Motivation's Influence on Salary:** This inherent motivation leads to\n    exceptional performance and subsequent salary raises, whether or not they\n    participate in the bootcamp.\n5.  **Matching Gone Awry:** By focusing on matching solely based on experience\n    and overlooking motivation, you inadvertently compare highly motivated\n    non-participants with a mix of motivated and less motivated participants.\n\nThe consequence? Your analysis paints a deceptive picture, indicating a negative\neffect of the bootcamp when the true effect is, in fact, positive.\n\nThis example illustrates a critical lesson in causal inference: the danger of\nunmeasured confounders. In this case, motivation acts as an unmeasured\nconfounder, influencing both the likelihood of bootcamp participation and salary\nincreases. As a business data scientist, this scenario highlights the importance\nof:\n\n1.  Thinking critically about all factors that might influence both your\n    treatment (bootcamp participation) and outcome (salary increases).\n2.  Recognizing the limitations of your data and analysis methods.\n3.  Communicating these nuances to stakeholders who might otherwise make\n    decisions based on misleading results.\n4.  Considering additional data collection or alternative analysis methods to\n    account for potential unmeasured confounders.\n\nIn the end, your role isn't just to crunch numbers, but to uncover the true\nstory behind the data and guide your company towards informed decisions. This\nmight involve recommending a more comprehensive study that includes measures of\nmotivation, or suggesting a randomized pilot program for future iterations of\nthe bootcamp.\n\n## Conclusion: The Power and Pitfalls of Matching\n\nMatching is a powerful tool in the causal inference toolkit, offering a way to\nconstruct valid comparison groups and tease out causal effects from\nobservational data. However, as we've seen, it's not without its complexities\nand potential pitfalls.\n\nFrom the basic concept of pairing similar units to the intricacies of different\ndistance measures and matching algorithms, we've explored the mechanics of how\nmatching works. We've also delved into its limitations, illustrated vividly by\nthe Ozzy Osbourne Conundrum, which reminds us that observable characteristics\ndon't always tell the full story.\n\nThe critique by King and Nielsen serves as a important cautionary tale,\nparticularly regarding the use of propensity score matching. Their work\nunderscores the importance of understanding the theoretical underpinnings of our\nmethods and approaching them critically.\n\nAs data scientists, our task is to navigate these complexities, understanding\nwhen and how to apply matching methods appropriately. We must be aware of their\nstrengths and limitations, always striving for transparency in our processes and\nrobustness in our results.\n\nMatching, when used judiciously, can be a powerful ally in our quest to uncover\ncausal relationships. But like any tool, its effectiveness depends on the skill\nand understanding of those who wield it. As we continue to push the boundaries\nof causal inference, let's carry forward this nuanced understanding of matching,\nalways remaining open to new developments and critiques that can refine our\nmethodological toolkit.\n\n::: {.callout-tip}\n## Learn more\n - @stuart2011matchit {MatchIt}: Nonparametric Preprocessing for Parametric Causal Inference.\n - @King_Nielsen_2019 Why Propensity Scores Should Not Be Used for Matching.\n:::",
    "supporting": [
      "matching_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}